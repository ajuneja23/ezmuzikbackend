{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXpOpgxvcZbs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('muse_v3.csv')"
      ],
      "metadata": {
        "id": "rfiL-acycv4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "PgELIdngemx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=data.drop(columns=['lastfm_url','artist','number_of_emotion_tags','valence_tags','arousal_tags','dominance_tags','mbid','spotify_id','genre'])\n",
        "data"
      ],
      "metadata": {
        "id": "gOQ2bNjofjyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=data['track'].values\n",
        "x"
      ],
      "metadata": {
        "id": "a7mzFjLE2Y8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=data['seeds'].values\n",
        "y"
      ],
      "metadata": {
        "id": "5m0Z7j952hOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiments=y\n",
        "emotions=[]\n",
        "for element in sentiments:\n",
        "  start_index=element.find(\"'\")+1\n",
        "  end_index=min(element.find(']')-1,element.find(',')-1)\n",
        "  emotions.append(element[start_index:end_index])\n",
        "count=0\n",
        "unique_emotions=[]\n",
        "for element in emotions:\n",
        "  if not element in unique_emotions:\n",
        "    unique_emotions.append(element)\n",
        "\n",
        "print(unique_emotions)"
      ],
      "metadata": {
        "id": "kJHBDtVRf2El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=data.rename(columns={'seeds': 'emotions'})\n",
        "data"
      ],
      "metadata": {
        "id": "b8_j2oqbil41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['emotions']=emotions\n",
        "data"
      ],
      "metadata": {
        "id": "3uk45LhFisu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_number_directory={}\n",
        "id=0\n",
        "for element in unique_emotions:\n",
        "  emotion_number_directory[element]=id\n",
        "  id+=1\n",
        "emotion_number_directory"
      ],
      "metadata": {
        "id": "UgAOvfiujC0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "id": "v9U2TbomjVpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "ALVKDRBKtf8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "tracks=data['track'].values\n",
        "processed=[]\n",
        "for track in tracks:\n",
        "  processed.append(track.lower())\n",
        "for i in range(len(processed)):\n",
        "  processed[i]=re.sub('[^a-zA-Z]',' ',processed[i])\n",
        "for i in range(len(processed)):\n",
        "  processed[i]=re.sub(r'\\s+',' ',processed[i])\n",
        "processed"
      ],
      "metadata": {
        "id": "W2bTpNB3tMXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "-8w-3hISus-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words=[nltk.word_tokenize(title) for title in processed]\n",
        "all_words"
      ],
      "metadata": {
        "id": "37f6x1CVvBMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "for i in range(len(all_words)):\n",
        "  all_words[i]=[w for w in all_words[i] if w not in stopwords.words('english')]\n",
        "all_words"
      ],
      "metadata": {
        "id": "3BQA6EyOvWQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "real_words=[]\n",
        "real_emotions=[]\n",
        "for i in range(len(all_words)):\n",
        "  if len(all_words[i])!=0:\n",
        "    real_words.append(all_words[i])\n",
        "    real_emotions.append(emotions[i])\n",
        "all_words=real_words\n",
        "emotions=real_emotions\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(all_words,emotions)"
      ],
      "metadata": {
        "id": "_V9nzlJ0-Ul1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " x_train"
      ],
      "metadata": {
        "id": "4Oxz0-DU-nbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "Lf5kHtSa-r4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "model = FastText(vector_size=10, window=3, min_count=1)\n",
        "model.build_vocab(corpus_iterable=x_train)\n",
        "model.train(corpus_iterable=x_train,total_examples=len(x_train),epochs=150)\n"
      ],
      "metadata": {
        "id": "_2yCp-XLlRWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv['mf']"
      ],
      "metadata": {
        "id": "FMLQ6i68yt4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(negative=['smart'])"
      ],
      "metadata": {
        "id": "2lEkRN-twh0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_length=len(model.wv)\n",
        "Embedding_dimensions=10"
      ],
      "metadata": {
        "id": "nl59xtc5Lm_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "ATM4JUYaRDIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "tokenizer.num_words = vocab_length\n",
        "print(\"Tokenizer vocab length:\", vocab_length)\n"
      ],
      "metadata": {
        "id": "_bn2AtkvRENB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "input_length = 60\n",
        "x_train=pad_sequences(tokenizer.texts_to_sequences(x_train),maxlen=input_length)\n",
        "x_test=pad_sequences(tokenizer.texts_to_sequences(x_test),maxlen=input_length)"
      ],
      "metadata": {
        "id": "M2LM800XRXWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "L4ePopB0RXY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "id": "xZHkp28wWQZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_length+2, Embedding_dimensions))\n",
        "for word, token in tokenizer.word_index.items():\n",
        "  if model.wv.__contains__(word):\n",
        "    embedding_matrix[token]=model.wv.__getitem__(word)\n",
        "embedding_matrix.shape"
      ],
      "metadata": {
        "id": "oUNvpFBqWaYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix"
      ],
      "metadata": {
        "id": "w1PueYP6bvDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding\n",
        "\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "metadata": {
        "id": "2Tn-EmEHAJLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural_net=Sequential()\n",
        "neural_net.add(Embedding(input_dim=vocab_length+2,output_dim=Embedding_dimensions,weights=[embedding_matrix],input_length=input_length))\n",
        "neural_net.add(Bidirectional(LSTM(100,dropout=0.3,return_sequences=True)))\n",
        "neural_net.add(Bidirectional(LSTM(100,dropout=0.3,return_sequences=True)))\n",
        "neural_net.add(Conv1D(100, 5, activation='relu'))\n",
        "neural_net.add(GlobalMaxPool1D())\n",
        "neural_net.add(Dense(16, activation='relu'))\n",
        "neural_net.add(Dense(273, activation='softmax'))"
      ],
      "metadata": {
        "id": "GYXWvqbefzfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural_net.summary()"
      ],
      "metadata": {
        "id": "jd4_jbUtYIgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural_net.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "P0Eg6gRsYshx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_y_train=[]\n",
        "for label in y_train:\n",
        "  one_hot_encoded=[]\n",
        "  for i in range(273):\n",
        "    if i==emotion_number_directory[label]:\n",
        "      one_hot_encoded.append(1)\n",
        "    else:\n",
        "      one_hot_encoded.append(0)\n",
        "  parsed_y_train.append(one_hot_encoded)\n",
        "parsed_y_train[0][133]"
      ],
      "metadata": {
        "id": "Jt2mTU78Z3k2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_y_test=[]\n",
        "for label in y_test:\n",
        "  one_hot_encoded=[]\n",
        "  for i in range(273):\n",
        "    if i==emotion_number_directory[label]:\n",
        "      one_hot_encoded.append(1)\n",
        "    else:\n",
        "      one_hot_encoded.append(0)\n",
        "  parsed_y_test.append(one_hot_encoded)"
      ],
      "metadata": {
        "id": "iSdT0Oq6iUK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=x_train.tolist()\n",
        "history=neural_net.fit(x_train,parsed_y_train,batch_size=32,epochs=3,verbose=True)"
      ],
      "metadata": {
        "id": "RkJEKB-9aBio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "1Llls0jwmJAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "sT_AIKJomJLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib"
      ],
      "metadata": {
        "id": "nyoiHPCAmQGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(neural_net,'nlpmodel.pkl')"
      ],
      "metadata": {
        "id": "j_VwrJ4xmkaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=neural_net.predict(x_test)"
      ],
      "metadata": {
        "id": "IV3Va9Inm6-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "ABFyGqNXhtyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "hO-jEQB8iCyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "predictions=[]\n",
        "for arr in y_pred:\n",
        "  prediction=tf.argmax(arr)\n",
        "  datapoint=[]\n",
        "  for i in range(273):\n",
        "    if i==prediction:\n",
        "      datapoint.append(1)\n",
        "    else:\n",
        "      datapoint.append(0)\n",
        "  predictions.append(datapoint)\n",
        "predictions"
      ],
      "metadata": {
        "id": "UBLZyODAh3Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions[0])"
      ],
      "metadata": {
        "id": "leLX9IDhmXnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_count=0\n",
        "correct_count=0\n",
        "for i in range(len(predictions)):\n",
        "  total_count+=1\n",
        "  match=True\n",
        "  for j in range(len(predictions[0])):\n",
        "    if predictions[i][j]!=parsed_y_test[i][j]:\n",
        "      match=False\n",
        "      break\n",
        "  if match:\n",
        "    correct_count+=1\n",
        "print(correct_count)\n",
        "print(total_count)\n"
      ],
      "metadata": {
        "id": "LLSQj4Zkmgzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def prediction_pipeline(title,tokenizer,emotion_number_directory,neural_net):\n",
        "  parsed_title=title.lower()\n",
        "  parsed_title=re.sub('[^a-zA-Z]',' ',parsed_title)\n",
        "  parsed_title=re.sub(r'\\s+',' ',parsed_title)\n",
        "  words=nltk.word_tokenize(parsed_title)\n",
        "  words=[w for w in words if w not in stopwords.words('english')]\n",
        "  sample=pad_sequences(tokenizer.texts_to_sequences([words]),maxlen=60)\n",
        "  prediction_array=neural_net.predict(sample)[0]\n",
        "  prediction_array=prediction_array.tolist()\n",
        "  prediction_array=[(prediction_array[i],i) for i in range(273)]\n",
        "  prediction_array.sort()\n",
        "  prediction_array.reverse()\n",
        "  prediction_array=prediction_array[:5]\n",
        "  for i in range(5):\n",
        "    target=prediction_array[i][1]\n",
        "    for key in emotion_number_directory.keys():\n",
        "      if emotion_number_directory[key]==target:\n",
        "        prediction_array[i]=(prediction_array[i][0],key)\n",
        "        break\n",
        "  return prediction_array"
      ],
      "metadata": {
        "id": "LtHSR259mjqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_pipeline('great day',tokenizer,emotion_number_directory,neural_net)\n"
      ],
      "metadata": {
        "id": "hcBlWnNWm6fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(prediction_pipeline,'pipeline.pkl')"
      ],
      "metadata": {
        "id": "0VrUa9Xx27nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(tokenizer,'tokenizer.pkl')"
      ],
      "metadata": {
        "id": "SWOczI623c8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(emotion_number_directory,'emotion_number_directory.pkl')"
      ],
      "metadata": {
        "id": "50uX_G5T33nz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}